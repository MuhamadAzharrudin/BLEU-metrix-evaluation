# BLEU Metric Evaluation

This repository contains Python scripts to calculate BLEU (Bilingual Evaluation Understudy) scores to assess the quality of automatic translation systems. BLEU scores are a popular metric for measuring how close a model's output is to a reference translation.

## ðŸŽ¯ Fitur
- Calculate BLEU score
- Supports **smoothing** for short output
- Evaluate on multiple reference & hypothesis pairs
- Input format: CSV/Excel
- Output: BLEU score and summary statistics

## ðŸ“¦ Installation
```bash
git clone https://github.com/MuhamadAzharrudin/BLEU-metrix-evaluation.git
